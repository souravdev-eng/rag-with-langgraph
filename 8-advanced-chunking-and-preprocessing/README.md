# üìÑ Section 8: Advanced Chunking & Preprocessing

---

## ‚ö° TL;DR (30-Second Summary)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  TRADITIONAL CHUNKING  ‚Üí  Fixed size splits     ‚Üí  "Cuts mid-thought" üòï    ‚îÇ
‚îÇ  SEMANTIC CHUNKING     ‚Üí  Meaning-based splits  ‚Üí  "Preserves context" ‚úÖ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

How it works:
  1. Split text into sentences
  2. Embed each sentence
  3. Group similar sentences (cosine similarity ‚â• threshold)
  4. Each group = one chunk
```

**One-liner:** Split by **meaning**, not by character count.

---

## üó∫Ô∏è How It Works (Visual)

```
                         INPUT TEXT
                              ‚îÇ
                              ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Split into     ‚îÇ
                    ‚îÇ   Sentences     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ                   ‚îÇ                   ‚îÇ
          ‚ñº                   ‚ñº                   ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ Sent 1  ‚îÇ        ‚îÇ Sent 2  ‚îÇ        ‚îÇ Sent 3  ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                   ‚îÇ                   ‚îÇ
          ‚ñº                   ‚ñº                   ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇEmbed [..]‚îÇ       ‚îÇEmbed [..]‚îÇ       ‚îÇEmbed [..]‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ                   ‚îÇ                   ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ                   ‚îÇ
                    ‚ñº                   ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ Similarity   ‚îÇ    ‚îÇ Similarity   ‚îÇ
            ‚îÇ   ‚â• 0.7?     ‚îÇ    ‚îÇ   ‚â• 0.7?     ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ                   ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ YES: Same   ‚îÇ     ‚îÇ NO: New     ‚îÇ
            ‚îÇ   Chunk     ‚îÇ     ‚îÇ   Chunk     ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìö Table of Contents

1. [Traditional vs Semantic Chunking](#1-traditional-vs-semantic-chunking)
2. [How Semantic Chunking Works](#2-how-semantic-chunking-works)
3. [Custom Semantic Chunker](#3-custom-semantic-chunker-from-scratch)
4. [LangChain SemanticChunker](#4-langchain-semanticchunker)
5. [RAG Pipeline Integration](#5-rag-pipeline-integration)
6. [Quick Reference Cheatsheet](#6-quick-reference-cheatsheet)
7. [Self-Test Questions](#7-self-test-questions)

---

## 1. Traditional vs Semantic Chunking

### üéØ The Problem with Traditional Chunking

```
Traditional (Character-based):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ "LangChain is a framework for building  ‚îÇ ‚Üê Chunk 1
‚îÇ applications with LLMs. Langchain prov" ‚îÇ ‚Üê CUT MID-WORD! üò±
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ "ides modular abstractions to combine"  ‚îÇ ‚Üê Chunk 2
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

```
Semantic (Meaning-based):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ "LangChain is a framework for building  ‚îÇ
‚îÇ applications with LLMs. Langchain       ‚îÇ ‚Üê Chunk 1 (complete thought)
‚îÇ provides modular abstractions..."       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ "The Eiffel Tower is located in Paris.  ‚îÇ ‚Üê Chunk 2 (different topic)
‚îÇ France is a popular tourist dest..."    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üìä Comparison Table

| Aspect           | Traditional Chunking        | Semantic Chunking              |
| ---------------- | --------------------------- | ------------------------------ |
| **Split Method** | Fixed character/token count | Embedding similarity           |
| **Context**      | ‚ùå May cut mid-sentence     | ‚úÖ Preserves complete thoughts |
| **Speed**        | ‚ö° Very fast                | üê¢ Slower (needs embeddings)   |
| **Quality**      | Lower retrieval quality     | Higher retrieval quality       |
| **Use Case**     | Simple docs, speed-critical | Complex docs, quality-critical |

### üí° Key Insight

> **Traditional chunking is blind to meaning.** It's like cutting a book into 500-word pieces without checking if you're mid-sentence. Semantic chunking **understands** where thoughts end.

---

## 2. How Semantic Chunking Works

### üîß The Algorithm

```
1. Split text into sentences
2. Embed each sentence using a model (e.g., all-MiniLM-L6-v2)
3. Compare adjacent sentence embeddings (cosine similarity)
4. If similarity ‚â• threshold ‚Üí Same chunk
5. If similarity < threshold ‚Üí Start new chunk
```

### üìù Core Code Pattern

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# Initialize
model = SentenceTransformer('all-MiniLM-L6-v2')
threshold = 0.7  # Tune this!

# Split into sentences
sentences = [s.strip() for s in text.split('.') if s.strip()]

# Embed all sentences
embeddings = model.encode(sentences)

# Group by similarity
chunks = []
current_chunk = [sentences[0]]

for i in range(1, len(sentences)):
    sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]

    if sim >= threshold:
        current_chunk.append(sentences[i])  # Same topic
    else:
        chunks.append(". ".join(current_chunk) + ".")  # New topic
        current_chunk = [sentences[i]]

chunks.append(". ".join(current_chunk) + ".")  # Don't forget last chunk!
```

### üéöÔ∏è Threshold Guide

```
threshold = 0.9  ‚Üí  Very strict (smaller, tighter chunks)
threshold = 0.7  ‚Üí  Balanced ‚úÖ (recommended)
threshold = 0.5  ‚Üí  Loose (larger chunks, more content together)
```

### üß† Memory Trick

> **"High threshold = High standards = Smaller chunks"**

---

## 3. Custom Semantic Chunker (From Scratch)

### üìù Reusable Class

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from langchain.schema import Document

class ThresholdSemanticChunker:
    def __init__(self, model_name="all-MiniLM-L6-v2", threshold=0.7):
        self.model = SentenceTransformer(model_name)
        self.threshold = threshold

    def split(self, text: str) -> list[str]:
        """Split raw text into semantic chunks."""
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        embeddings = self.model.encode(sentences)

        chunks = []
        current_chunk = [sentences[0]]

        for i in range(1, len(sentences)):
            sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]
            if sim >= self.threshold:
                current_chunk.append(sentences[i])
            else:
                chunks.append(". ".join(current_chunk) + ".")
                current_chunk = [sentences[i]]

        chunks.append(". ".join(current_chunk) + ".")
        return chunks

    def split_documents(self, docs: list[Document]) -> list[Document]:
        """Split LangChain Documents into semantic chunks."""
        result = []
        for doc in docs:
            for chunk in self.split(doc.page_content):
                result.append(Document(page_content=chunk, metadata=doc.metadata))
        return result
```

### üöÄ Usage

```python
# Initialize
chunker = ThresholdSemanticChunker(threshold=0.7)

# From raw text
chunks = chunker.split("Your long text here...")

# From LangChain Documents
doc = Document(page_content="Your text here...")
chunk_docs = chunker.split_documents([doc])
```

---

## 4. LangChain SemanticChunker

### üéØ Built-in Solution

LangChain provides `SemanticChunker` in the experimental module.

### üìù Implementation

```python
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader

# Load documents
loader = TextLoader("your_file.txt")
docs = loader.load()

# Initialize embedding model
embedding = OpenAIEmbeddings()

# Create semantic chunker
chunker = SemanticChunker(embedding)

# Split documents
chunks = chunker.split_documents(docs)

# View results
for i, chunk in enumerate(chunks):
    print(f"\nChunk {i+1}:\n{chunk.page_content}")
```

### ‚öñÔ∏è Custom vs LangChain

| Aspect           | Custom Chunker              | LangChain SemanticChunker |
| ---------------- | --------------------------- | ------------------------- |
| **Control**      | Full control over algorithm | Less customizable         |
| **Dependencies** | sentence-transformers       | langchain-experimental    |
| **Embedding**    | Any SentenceTransformer     | Any LangChain Embeddings  |
| **Use When**     | Need custom logic/threshold | Quick prototyping         |

---

## 5. RAG Pipeline Integration

### üîß Complete Pipeline

```python
from langchain_community.vectorstores import FAISS, Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1Ô∏è‚É£ Load & Chunk (Semantic)
chunker = ThresholdSemanticChunker(threshold=0.7)
chunks = chunker.split_documents(docs)

# 2Ô∏è‚É£ Create Vector Store
embedding = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(chunks, embedding)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# 3Ô∏è‚É£ Prompt Template
template = """Answer based on the context:

Context: {context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)

# 4Ô∏è‚É£ LLM
llm = ChatOpenAI(model="gpt-4o-mini")

# 5Ô∏è‚É£ RAG Chain (LCEL)
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 6Ô∏è‚É£ Query
result = rag_chain.invoke("What is LangChain used for?")
```

### üó∫Ô∏è Pipeline Flow

```
Document ‚Üí Semantic Chunker ‚Üí Vector Store ‚Üí Retriever ‚Üí RAG Chain ‚Üí Answer
              (split by         (FAISS/       (top-k)    (prompt +
               meaning)          Chroma)                   LLM)
```

---

## 6. Quick Reference Cheatsheet

### üì¶ All Imports

```python
# Custom Semantic Chunking
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# LangChain Semantic Chunking
from langchain_experimental.text_splitter import SemanticChunker
from langchain_openai import OpenAIEmbeddings

# Document Handling
from langchain.schema import Document
from langchain_community.document_loaders import TextLoader

# Vector Stores
from langchain_community.vectorstores import FAISS, Chroma
```

### üîë Copy-Paste Snippets

**Custom Chunker (Quick):**

```python
chunker = ThresholdSemanticChunker(threshold=0.7)
chunks = chunker.split_documents(docs)
```

**LangChain Chunker (Quick):**

```python
chunker = SemanticChunker(OpenAIEmbeddings())
chunks = chunker.split_documents(docs)
```

**Cosine Similarity:**

```python
sim = cosine_similarity([embedding1], [embedding2])[0][0]
```

### üöÄ Decision Tree

```
Need fast chunking?
  ‚îî‚îÄ YES ‚Üí RecursiveCharacterTextSplitter (traditional)
  ‚îî‚îÄ NO  ‚Üí Need semantic coherence?
            ‚îî‚îÄ YES ‚Üí SemanticChunker
            ‚îî‚îÄ CUSTOM ‚Üí ThresholdSemanticChunker
```

---

## üéì Key Takeaways

| Concept         | Purpose                   | Key Code                        |
| --------------- | ------------------------- | ------------------------------- |
| **Traditional** | Fast, simple splits       | `CharacterTextSplitter()`       |
| **Semantic**    | Meaning-preserving splits | `SemanticChunker(embedding)`    |
| **Threshold**   | Control chunk granularity | `threshold=0.7` (tune it!)      |
| **Similarity**  | Compare sentence meanings | `cosine_similarity(emb1, emb2)` |

---

## 7. Self-Test Questions

### üìù Quick Recall (Click to reveal!)

<details>
<summary><b>1. What's the main problem with traditional character-based chunking?</b></summary>

It **cuts text blindly** without considering meaning. You might split mid-sentence, mid-word, or separate related information into different chunks. This hurts retrieval quality because context is lost.

</details>

<details>
<summary><b>2. How does semantic chunking decide where to split?</b></summary>

1. Split text into sentences
2. Create embeddings for each sentence
3. Calculate cosine similarity between adjacent sentences
4. If similarity ‚â• threshold ‚Üí keep in same chunk
5. If similarity < threshold ‚Üí start new chunk

</details>

<details>
<summary><b>3. What does the threshold parameter control?</b></summary>

How similar sentences must be to stay in the same chunk:

- **Higher threshold (0.9)** = Stricter, smaller chunks, only very similar sentences grouped
- **Lower threshold (0.5)** = Looser, larger chunks, more content together
- **Recommended: 0.7** = Balanced

</details>

<details>
<summary><b>4. What embedding model is commonly used for semantic chunking?</b></summary>

`all-MiniLM-L6-v2` from SentenceTransformers - it's fast, lightweight, and produces good quality embeddings for similarity comparisons.

</details>

<details>
<summary><b>5. What's cosine similarity and why use it?</b></summary>

Cosine similarity measures the angle between two vectors (embeddings). Range: -1 to 1.

- **1.0** = Identical direction (same meaning)
- **0.0** = Perpendicular (unrelated)
- **-1.0** = Opposite

We use it because it's **scale-invariant** - it measures direction, not magnitude, which is better for comparing text meanings.

</details>

<details>
<summary><b>6. When should you use traditional chunking vs semantic chunking?</b></summary>

**Traditional:**

- Speed is critical
- Simple documents
- Prototyping quickly

**Semantic:**

- Quality matters
- Complex documents with multiple topics
- When retrieval accuracy is important

</details>

<details>
<summary><b>7. What's the difference between custom ThresholdSemanticChunker and LangChain's SemanticChunker?</b></summary>

**Custom ThresholdSemanticChunker:**

- Full control over threshold and algorithm
- Uses SentenceTransformers directly
- More customizable

**LangChain SemanticChunker:**

- Easy to use, less code
- Integrates with any LangChain embedding model
- Less customizable but good for quick prototyping

</details>

---

### üéØ Code Challenge

**Try writing this from memory:**

> Create a semantic chunker that splits text based on embedding similarity, then use it in a simple RAG pipeline.

<details>
<summary><b>Solution</b></summary>

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# Custom Semantic Chunker
class SemanticChunker:
    def __init__(self, threshold=0.7):
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.threshold = threshold

    def split(self, text):
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        embeddings = self.model.encode(sentences)
        chunks, current = [], [sentences[0]]

        for i in range(1, len(sentences)):
            sim = cosine_similarity([embeddings[i-1]], [embeddings[i]])[0][0]
            if sim >= self.threshold:
                current.append(sentences[i])
            else:
                chunks.append(". ".join(current) + ".")
                current = [sentences[i]]
        chunks.append(". ".join(current) + ".")
        return [Document(page_content=c) for c in chunks]

# RAG Pipeline
chunker = SemanticChunker(threshold=0.7)
chunks = chunker.split("Your long document text here...")

vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())
retriever = vectorstore.as_retriever()

prompt = ChatPromptTemplate.from_template("Context: {context}\n\nQuestion: {question}")
llm = ChatOpenAI(model="gpt-4o-mini")

chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt | llm | StrOutputParser()
)

answer = chain.invoke("Your question here")
```

</details>

---

## üìÅ Files in This Section

| File                     | Description                       |
| ------------------------ | --------------------------------- |
| `semanti_chunking.ipynb` | Semantic chunking implementations |
| `langchain_intro.txt`    | Sample text for testing chunkers  |

---

> üí° **Golden Rule:** Use **semantic chunking** when retrieval quality matters. The extra computation cost pays off in better RAG answers!
