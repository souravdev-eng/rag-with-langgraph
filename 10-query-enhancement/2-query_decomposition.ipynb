{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e7d68c",
   "metadata": {},
   "source": [
    "### ðŸ§  What is Query Decomposition?\n",
    "Query decomposition is the process of taking a complex, multi-part question and breaking it into simpler, atomic sub-questions that can each be retrieved and answered individually.\n",
    "\n",
    "#### âœ… Why Use Query Decomposition?\n",
    "\n",
    "- Complex queries often involve multiple concepts\n",
    "\n",
    "- LLMs or retrievers may miss parts of the original question\n",
    "\n",
    "- It enables multi-hop reasoning (answering in steps)\n",
    "\n",
    "- Allows parallelism (especially in multi-agent frameworks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23a442b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sauravmajumdar/Developer/AI/RAG/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76de0145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and embed the document\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"lambda_mult\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f43149d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={'max_input_tokens': 128000, 'max_output_tokens': 16384, 'image_inputs': True, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x3196627b0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x319f1b200>, root_client=<openai.OpenAI object at 0x31872c0e0>, root_async_client=<openai.AsyncOpenAI object at 0x319662810>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(model=\"openai:gpt-4o-mini\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "# decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "# You are an AI assistant. Decompose the following complex question into 2 to 4 smaller sub-questions for better document retrieval.\n",
    "\n",
    "# Question: \"{question}\"\n",
    "\n",
    "# Sub-questions:\n",
    "# \"\"\")\n",
    "# decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eabed0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query decomposition\n",
    "decomposition_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Iâ€™m going to ask you a question. I want you to decompose it into a series of subquestions. Each subquestion should be self-contained with all the information necessary to solve it.\n",
    "\n",
    "Make sure not to decompose more than necessary or have any trivial subquestions - youâ€™ll be evaluated on the simplicity, conciseness, and correctness of your decompositions as well as your final answer. You should wrap each subquestion in <sub q></sub q> tags. After each subquestion, you should answer the subquestion and put your subanswer in <sub a></sub a> tags.\n",
    "\n",
    " Once you have all the information you need to answer the question, output <FIN></FIN> tags.\n",
    "\n",
    "example:\n",
    "Question: What is Bitcoin?\n",
    "<sub q>What is the purpose of Bitcoin?</sub q>\n",
    "<sub a>Bitcoin serves as a decentralized digital currency.</sub a>\n",
    "<sub q>What does decentralized mean?</sub q>\n",
    "<sub a>Decentralized means it operates without a central authority or single administrator.</sub a>\n",
    "<FIN>Bitcoin is a decentralized digital currency that operates without a central authority.</FIN>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "decomposition_chain = decomposition_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c9797ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "decomposition_question=decomposition_chain.invoke({\"question\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b4819b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sub q>What is LangChain's approach to memory in AI applications?</sub q>  \n",
      "<sub a>LangChain uses memory to store and recall information pertinent to user interactions and tasks, allowing for context-aware responses and improved conversation flow.</sub a>  \n",
      "\n",
      "<sub q>How does LangChain incorporate agents in its architecture?</sub q>  \n",
      "<sub a>LangChain incorporates agents as autonomous entities that can interact with external tools or APIs, execute tasks, and respond to user queries based on defined objectives.</sub a>  \n",
      "\n",
      "<sub q>What does CrewAI offer in terms of memory management for AI?</sub q>  \n",
      "<sub a>CrewAI implements memory management by allowing AI assistants to learn from interactions and retain insights, enhancing long-term user engagement and personalization.</sub a>  \n",
      "\n",
      "<sub q>What is the role of agents in CrewAI's system?</sub q>  \n",
      "<sub a>In CrewAI, agents are designed to collaborate and coordinate tasks among themselves, thus enabling a more flexible and dynamic assistance approach across various applications.</sub a>  \n",
      "\n",
      "<sub q>How do LangChain and CrewAI differ in their use of memory?</sub q>  \n",
      "<sub a>LangChain focuses on contextual memory for ongoing interactions, while CrewAI emphasizes long-term learning and retention from user interactions over time.</sub a>  \n",
      "\n",
      "<sub q>How do LangChain and CrewAI differ in their utilization of agents?</sub q>  \n",
      "<sub a>LangChain's agents are mostly task-oriented with specific functionalities, while CrewAI's agents are more collaborative and capable of working together across tasks to achieve goals.</sub a>  \n",
      "\n",
      "<FIN>LangChain uses memory for context-aware interactions and agents for task execution, while CrewAI focuses on long-term learning and collaborative agents.</FIN>\n"
     ]
    }
   ],
   "source": [
    "print(decomposition_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5be04719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: QA chain per sub-question\n",
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Use the context below to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "qa_chain = create_stuff_documents_chain(llm=llm, prompt=qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26c735b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline logic\n",
    "def full_query_decomposition_rag_pipeline(user_query):\n",
    "    # Decompose the query\n",
    "    sub_qs_text = decomposition_chain.invoke({\"question\": user_query})\n",
    "    sub_questions = [q.strip(\"-â€¢1234567890. \").strip() for q in sub_qs_text.split(\"\\n\") if q.strip()]\n",
    "    \n",
    "    results = []\n",
    "    for subq in sub_questions:\n",
    "        docs = retriever.invoke(subq)\n",
    "        result = qa_chain.invoke({\"input\": subq, \"context\": docs})\n",
    "        results.append(f\"Q: {subq}\\nA: {result}\")\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac50f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final Answer:\n",
      "\n",
      "Q: <sub q>What is memory in the context of LangChain and CrewAI?</sub q>\n",
      "A: In the context of LangChain and CrewAI, memory refers to the capability of the system to retain and utilize information from previous interactions within a conversation or task. LangChain offers specific memory modules, such as ConversationBufferMemory and ConversationSummaryMemory, which enable the language model (LLM) to keep track of past conversation turns or summarize lengthy interactions to remain within token limits. This allows for enhanced awareness and context during interactions.\n",
      "\n",
      "In conjunction with CrewAI, which supports role-based collaboration and works with LangChain's tools, memory facilitates more effective communication and decision-making within hybrid systems. Together, these memory features help maintain context and continuity, enabling agents to plan and execute tasks dynamically based on prior information.\n",
      "\n",
      "Q: <sub a>Memory refers to the capability of a system to retain information from interactions, allowing it to provide more contextually relevant responses over time.</sub a>\n",
      "A: The statement provided in the question describes the function of memory in a system like LangChain, indicating that memory allows for the retention of information from interactions. This capability facilitates the generation of contextually relevant responses over time, which aligns with the purpose of memory modules such as ConversationBufferMemory and ConversationSummaryMemory mentioned in the context. \n",
      "\n",
      "Thus, the answer to the question is that the description accurately reflects the role of memory in enhancing the interactions of the language model by maintaining awareness of previous conversation turns and summarizing interactions.\n",
      "\n",
      "Q: <sub q>How does LangChain implement memory functions?</sub q>\n",
      "A: LangChain implements memory functions through memory modules like ConversationBufferMemory and ConversationSummaryMemory. These modules enable the language model (LLM) to retain awareness of previous conversation turns, allowing it to maintain context during interactions. Additionally, they facilitate summarizing longer interactions to ensure they fit within token limits. This memory capability is essential for enhancing the flow of conversations and maintaining relevant context across multiple exchanges.\n",
      "\n",
      "Q: <sub a>LangChain uses structured memory to store user interactions and relevant data, enabling it to reference past interactions when generating responses.</sub a>\n",
      "A: The statement is true. LangChain utilizes memory modules like ConversationBufferMemory and ConversationSummaryMemory to store user interactions and relevant data. This structured memory allows the LLM to reference past interactions when generating responses, helping to maintain contextual awareness throughout a conversation.\n",
      "\n",
      "Q: <sub q>How does CrewAI implement memory functions?</sub q>\n",
      "A: The provided context does not specifically mention how CrewAI implements memory functions. It focuses on the capabilities of CrewAI, such as its support for multiple LLM backends, streaming, parallel execution, and asynchronous tool invocation, as well as its use in multi-step workflows. If you would like information on memory functions, it may be necessary to consult additional resources or documentation related to CrewAI.\n",
      "\n",
      "Q: <sub a>CrewAI employs a context-based memory system that prioritizes immediate interaction context while learning and adapting to user preferences over time.</sub a>\n",
      "A: The provided context does not directly confirm or detail a context-based memory system that prioritizes immediate interaction context while learning and adapting to user preferences over time. The focus is primarily on the innovative features of agent context-sharing and how it facilitates collaboration among agents in complex workflows. If you have more specific information or details about CrewAI's memory system, please provide it for a more tailored response.\n",
      "\n",
      "Q: <sub q>What are agents in the context of LangChain?</sub q>\n",
      "A: In the context of LangChain, agents are systems that operate using a planner-executor model. They utilize large language models (LLMs) to reason about which tools to call, what inputs to provide, and how to process the outputs. Agents are capable of executing multi-step tasks and can incorporate dynamic decision-making, branching logic, and context-aware memory across their operations. This allows them to plan and execute a sequence of tool invocations to achieve specific goals, integrating with various tools such as web searches, calculators, and code execution.\n",
      "\n",
      "Q: <sub a>Agents in LangChain refer to autonomous entities that can act on behalf of users by performing tasks or making decisions based on predefined goals and available information.</sub a>\n",
      "A: The statement provided in the question correctly defines agents in LangChain. They are indeed autonomous entities that act on behalf of users, capable of performing tasks and making decisions based on predefined goals and the information available to them. This fits well with the context, which highlights the planner-executor model of LangChain agents, their ability to reason about tool usage, and their integration with various tools to achieve specific objectives.\n",
      "\n",
      "Q: <sub q>What are agents in the context of CrewAI?</sub q>\n",
      "A: In the context of CrewAI, agents are entities that are defined by a specific purpose, a goal, and a set of tools they can utilize. The framework is designed to ensure that each agent remains focused on their assigned tasks and contributes significantly to the overall objectives of the crew.\n",
      "\n",
      "Q: <sub a>Agents in CrewAI are designed to facilitate communication, manage tasks, and automate processes based on user needs and preferences, often using AI-driven decision-making.</sub a>\n",
      "A: The statement in the question refers to the functionalities of agents in CrewAI. While the context provided does not explicitly mention facilitating communication or managing tasks in detail, it does highlight that CrewAI agents are semi-independent and work collaboratively within structured workflows, with defined roles and purposes. This implies a level of task management and automation aligned with user needs, likely supported by AI-driven decision-making. Therefore, while the context does not directly confirm the specific functionalities mentioned, it aligns with the overarching concepts of CrewAI regarding agent collaboration and workflow management.\n",
      "\n",
      "Q: <sub q>How do memory functions differ between LangChain and CrewAI?</sub q>\n",
      "A: The memory functions in LangChain and CrewAI differ primarily in their focus and implementation. LangChain offers memory modules such as ConversationBufferMemory and ConversationSummaryMemory, which are specifically designed to help the language model (LLM) maintain awareness of previous conversation turns or to summarize long interactions to fit within token limits. This allows for a more continuous and coherent dialogue experience.\n",
      "\n",
      "On the other hand, CrewAI does not focus on memory functions in the same way; instead, it integrates with LangChain to enhance collaboration through role-based management. CrewAI supports this by leveraging LangChain's memory capabilities but does not have its own distinct memory functions. In summary, while LangChain provides specific memory functionalities to manage conversational context, CrewAI focuses on facilitating collaboration using the memory features offered by LangChain.\n",
      "\n",
      "Q: <sub a>LangChain focuses on structured, long-term memory suitable for context-rich interactions, while CrewAI emphasizes immediate context with adaptability to user preferences.</sub a>\n",
      "A: The statement you provided compares the focus of LangChain and CrewAI. LangChain indeed emphasizes structured long-term memory, which enables it to manage the context of conversations effectively, such as through modules like ConversationBufferMemory and ConversationSummaryMemory. These capabilities allow LangChain to retain awareness of previous interactions or summarize them as needed.\n",
      "\n",
      "On the other hand, CrewAI is centered around role-based collaboration, suggesting that it prioritizes immediate contextual understanding and adaptability to the user's preferences, facilitating dynamic collaboration among team members.\n",
      "\n",
      "Overall, the two systems complement each other: LangChain provides a robust structure for remembering and summarizing interactions, while CrewAI focuses on enhancing immediate collaboration through adaptable context.\n",
      "\n",
      "Q: <sub q>How do agents differ between LangChain and CrewAI?</sub q>\n",
      "A: Agents in LangChain primarily focus on handling retrieval and tool wrapping, which involves pulling relevant information and managing tools for various tasks. In contrast, CrewAI agents are defined with a specific purpose and goal, and they utilize a set of tools to ensure meaningful contributions toward the overall crew objective. Additional to this, CrewAI emphasizes role-based collaboration among agents, which distinguishes it from the more retrieval-focused approach of LangChain.\n",
      "\n",
      "Q: <sub a>LangChain's agents are more focused on executing specific tasks based on user goals, while CrewAI's agents prioritize managing workflows and enhancing user communication.</sub a>\n",
      "A: The statement highlights the distinction between LangChain's agents and CrewAI's agents. LangChain's agents are designed to execute specific tasks aligned with user goals, emphasizing task completion. In contrast, CrewAI's agents are focused on managing workflows and improving communication among users, which suggests a broader role in facilitating collaboration and coordination within a team. This differentiation underscores the strengths of each framework in their respective domains.\n",
      "\n",
      "Q: <FIN>LangChain uses structured memory to enhance context-rich interactions and features agents for task execution, while CrewAI utilizes context-based memory for immediate adaptability and has agents focused on communication and workflow management.</FIN>\n",
      "A: The comparison highlights the different approaches of LangChain and CrewAI in enhancing interactions and managing tasks. LangChain employs structured memory, which helps in maintaining context-rich interactions and features agents designed for executing tasks. In contrast, CrewAI leverages context-based memory, allowing for immediate adaptability and focusing its agents on facilitating communication and managing workflows. This distinction underlines how each system is tailored for specific operational needs: LangChain for structured task execution and CrewAI for collaboration and workflow optimization.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run\n",
    "query = \"How does LangChain use memory and agents compared to CrewAI?\"\n",
    "final_answer = full_query_decomposition_rag_pipeline(query)\n",
    "print(\"âœ… Final Answer:\\n\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04921ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
