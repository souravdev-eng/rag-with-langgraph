# ğŸ” Section 9: Hybrid Search Strategies

---

## âš¡ TL;DR (30-Second Summary)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SPARSE (BM25)     â†’  Keyword matching      â†’  "Find exact words"          â”‚
â”‚  DENSE (FAISS)     â†’  Semantic embeddings   â†’  "Understand meaning"        â”‚
â”‚  HYBRID            â†’  Sparse + Dense        â†’  "Best of both"              â”‚
â”‚  RERANKING         â†’  Re-score top results  â†’  "Precision boost"           â”‚
â”‚  MMR               â†’  Diversity filter      â†’  "No redundant docs"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**One-liner:** Combine retrievers (Hybrid) â†’ Remove duplicates (MMR) â†’ Rerank for precision (FlashRank)

---

## ğŸ—ºï¸ How It All Connects

```
                              USER QUERY
                                  â”‚
                                  â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚                                       â”‚
              â–¼                                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  SPARSE  â”‚                           â”‚  DENSE   â”‚
        â”‚  (BM25)  â”‚                           â”‚ (FAISS)  â”‚
        â”‚ Keywords â”‚                           â”‚ Semantic â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
              â”‚                                       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ HYBRID/ENSEMBLE â”‚  â† Combine with weights [0.7, 0.3]
                    â”‚    Retriever    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚      MMR        â”‚  â† Optional: Remove redundant docs
                    â”‚   (Diversity)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   RERANKING     â”‚  â† Optional: Precision boost
                    â”‚  (FlashRank)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                        TOP-K DOCS â†’ LLM â†’ ANSWER
```

---

## ğŸ“š Table of Contents

1. [Dense vs Sparse Retrieval](#1-dense-vs-sparse-retrieval)
2. [Hybrid Retriever (Ensemble)](#2-hybrid-retriever-ensemble)
3. [Reranking Techniques](#3-reranking-techniques)
4. [MMR - Maximal Marginal Relevance](#4-mmr---maximal-marginal-relevance)
5. [Quick Reference Cheatsheet](#5-quick-reference-cheatsheet)
6. [Self-Test Questions](#6-self-test-questions)

---

## 1. Dense vs Sparse Retrieval

### ğŸ¯ Core Concept

| Aspect         | **Sparse (BM25)**                    | **Dense (Embeddings)**                |
| -------------- | ------------------------------------ | ------------------------------------- |
| **How**        | Keyword/TF-IDF scoring               | Vector similarity                     |
| **Strengths**  | âœ… Exact matches, Fast, No ML needed | âœ… Semantic meaning, Handles synonyms |
| **Weaknesses** | âŒ Misses synonyms                   | âŒ May miss exact terms               |
| **Best For**   | Code, Technical docs                 | Conversational, Natural language      |

### ğŸ’¡ Key Insight

> **Neither is perfect alone!** Sparse = exact terms, Dense = meaning. **Combine both.**

### ğŸ§  Memory Trick

> **"Dense = Deep meaning, Sparse = Surface keywords"**

---

## 2. Hybrid Retriever (Ensemble)

### ğŸ¯ What Is It?

Combines Dense + Sparse retrievers with weighted scores.

### ğŸ“ Code Pattern

```python
from langchain_community.retrievers import BM25Retriever
from langchain_community.vectorstores import FAISS
from langchain_classic.retrievers import EnsembleRetriever
from langchain_huggingface import HuggingFaceEmbeddings

# Dense
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
dense_retriever = FAISS.from_documents(docs, embedding_model).as_retriever(search_kwargs={"k": 3})

# Sparse
sparse_retriever = BM25Retriever.from_documents(docs)
sparse_retriever.k = 3

# Hybrid
hybrid = EnsembleRetriever(
    retrievers=[dense_retriever, sparse_retriever],
    weights=[0.7, 0.3]  # Tune these!
)
```

### âš–ï¸ Weight Guidelines

| Use Case           | Dense | Sparse |
| ------------------ | ----- | ------ |
| Conversational/NLP | 0.7   | 0.3    |
| Balanced           | 0.5   | 0.5    |
| Code/Exact terms   | 0.3   | 0.7    |

---

## 3. Reranking Techniques

### ğŸ¯ What Is It?

**Two-stage process:** Fast retrieval â†’ Accurate re-scoring

```
Query â†’ [Fast Retriever] â†’ Top-K â†’ [Reranker] â†’ Best Docs
```

### ğŸ”„ Why Rerank?

- Fast retrievers sacrifice accuracy for speed
- Vector similarity â‰  actual relevance
- Cross-encoders understand query-doc pairs better

---

### ğŸ“Œ Method 1: LLM-Based Reranking

```python
prompt = PromptTemplate.from_template("""
Rank these documents by relevance to: "{question}"

Documents:
{documents}

Output: comma-separated indices (e.g., 2,1,3,0)
""")

chain = prompt | llm | StrOutputParser()
response = chain.invoke({"question": query, "documents": formatted_docs})

# Parse and reorder
indices = [int(x.strip()) - 1 for x in response.split(",") if x.strip().isdigit()]
reranked = [docs[i] for i in indices if 0 <= i < len(docs)]
```

**Pros:** Flexible | **Cons:** Slow, expensive

---

### ğŸ“Œ Method 2: FlashRank â­ (Recommended)

Fast cross-encoder reranking without LLM costs.

```python
from langchain_community.document_compressors import FlashrankRerank
from langchain_classic.retrievers.contextual_compression import ContextualCompressionRetriever

compressor = FlashrankRerank(model="ms-marco-MiniLM-L-12-v2")
reranking_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

docs = reranking_retriever.invoke(query)
```

**Pros:** Fast, accurate, free | **Cons:** Model download needed

---

### ğŸ“‹ Compressors Comparison

| Compressor          | Speed   | Accuracy | Cost |
| ------------------- | ------- | -------- | ---- |
| `FlashrankRerank`   | âš¡ Fast | âœ… High  | Free |
| `CohereRerank`      | Medium  | âœ… High  | API  |
| `LLMChainExtractor` | ğŸ¢ Slow | High     | LLM  |
| `EmbeddingsFilter`  | âš¡ Fast | Medium   | Free |

### ğŸ§  Memory Trick

> **"Retrieve broad, Rerank narrow"**

---

## 4. MMR - Maximal Marginal Relevance

### ğŸ¯ What Is It?

Balances **relevance** + **diversity** to avoid redundant results.

### ğŸ¨ The Problem

```
Without MMR:                          With MMR:
1. Python is a language...            1. Python is a language...
2. Python is a programming lang...    2. Python has ML libraries...
3. Python language was created...     3. Python uses indentation...
   â†‘ REDUNDANT!                          â†‘ DIVERSE!
```

### ğŸ“ Implementation

```python
retriever = vectorstore.as_retriever(
    search_type="mmr",           # ğŸ”‘ Magic switch
    search_kwargs={
        "k": 5,                  # Final docs to return
        "fetch_k": 20,           # Candidates to consider
        "lambda_mult": 0.5       # 0=diversity, 1=relevance
    }
)
```

### ğŸšï¸ Lambda Guide

```
lambda_mult = 1.0  â†’  Pure relevance (no diversity)
lambda_mult = 0.5  â†’  Balanced âœ… (default)
lambda_mult = 0.0  â†’  Max diversity (may hurt relevance)
```

### ğŸ§  Memory Trick

> **"MMR = Maximum info, Minimum repetition"**

---

## 5. Quick Reference Cheatsheet

### ğŸš€ Decision Tree

```
Need exact keywords?
  â””â”€ YES â†’ BM25 (Sparse)
  â””â”€ NO  â†’ Need semantic?
            â””â”€ YES â†’ Dense (FAISS)
            â””â”€ BOTH â†’ Hybrid (EnsembleRetriever)

Results redundant?
  â””â”€ YES â†’ Add MMR (search_type="mmr")

Need precision?
  â””â”€ YES â†’ Add Reranking (FlashRank)
```

### ğŸ“¦ All Imports

```python
# Dense
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# Sparse
from langchain_community.retrievers import BM25Retriever

# Hybrid
from langchain_classic.retrievers import EnsembleRetriever

# Reranking
from langchain_community.document_compressors import FlashrankRerank
from langchain_classic.retrievers.contextual_compression import ContextualCompressionRetriever
```

### ğŸ”‘ Copy-Paste Snippets

**Hybrid:**

```python
EnsembleRetriever(retrievers=[dense, sparse], weights=[0.7, 0.3])
```

**MMR:**

```python
vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5})
```

**Reranking:**

```python
ContextualCompressionRetriever(base_compressor=FlashrankRerank(), base_retriever=retriever)
```

---

## ğŸ“ Key Takeaways

| Technique  | Purpose             | Key Code                                |
| ---------- | ------------------- | --------------------------------------- |
| **Sparse** | Keyword matching    | `BM25Retriever.from_documents()`        |
| **Dense**  | Semantic similarity | `FAISS.from_documents()`                |
| **Hybrid** | Combine both        | `EnsembleRetriever(weights=[0.7, 0.3])` |
| **MMR**    | Reduce redundancy   | `search_type="mmr"`                     |
| **Rerank** | Precision boost     | `FlashrankRerank()`                     |

---

## 6. Self-Test Questions

### ğŸ“ Quick Recall (Answer in your head!)

<details>
<summary><b>1. What's the difference between Dense and Sparse retrieval?</b></summary>

**Sparse (BM25):** Keyword/term matching using TF-IDF style scoring. Great for exact matches.

**Dense (Embeddings):** Semantic similarity in vector space. Understands meaning & synonyms.

</details>

<details>
<summary><b>2. Why combine Dense + Sparse in a Hybrid retriever?</b></summary>

Neither is perfect alone! Sparse catches exact keywords, Dense handles paraphrases and semantic meaning. Together they improve both precision and recall.

</details>

<details>
<summary><b>3. What does `weights=[0.7, 0.3]` mean in EnsembleRetriever?</b></summary>

Dense retriever gets 70% weight, Sparse gets 30%. Higher weight = more influence on final ranking. Use higher dense weight for conversational queries, higher sparse for technical/code search.

</details>

<details>
<summary><b>4. What is reranking and why use it?</b></summary>

Two-stage process: Fast retriever gets top-k docs â†’ Reranker re-scores them for precision. Vector similarity isn't always actual relevance. Cross-encoders (like FlashRank) understand query-document pairs better.

</details>

<details>
<summary><b>5. FlashRank vs LLM reranking - which is better?</b></summary>

**FlashRank:** Faster, cheaper (no API costs), uses cross-encoder models.

**LLM:** More flexible reasoning, but slower and expensive.

**Recommendation:** FlashRank for most cases.

</details>

<details>
<summary><b>6. What problem does MMR solve?</b></summary>

Reduces **redundancy** in retrieved documents. Without MMR, you might get 5 docs saying the same thing. MMR balances relevance with diversity to get complementary information.

</details>

<details>
<summary><b>7. What does `lambda_mult` control in MMR?</b></summary>

Balance between relevance (1.0) and diversity (0.0).

- `1.0` = Pure relevance, no diversity
- `0.5` = Balanced (recommended)
- `0.0` = Maximum diversity

</details>

<details>
<summary><b>8. What's the typical RAG pipeline order with these techniques?</b></summary>

```
Query â†’ Hybrid Retriever â†’ MMR (optional) â†’ Reranking (optional) â†’ Top-K Docs â†’ LLM
```

</details>

---

### ğŸ¯ Code Challenge

**Try writing this from memory:**

> Create a hybrid retriever that combines FAISS and BM25, then add MMR for diversity.

<details>
<summary><b>Solution</b></summary>

```python
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever
from langchain_huggingface import HuggingFaceEmbeddings

# Dense
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embeddings)
dense = vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 3})  # MMR here!

# Sparse
sparse = BM25Retriever.from_documents(docs)
sparse.k = 3

# Hybrid
hybrid = EnsembleRetriever(
    retrievers=[dense, sparse],
    weights=[0.7, 0.3]
)
```

</details>

---

## ğŸ“ Files in This Section

| File                        | Description                    |
| --------------------------- | ------------------------------ |
| `1-densesparse.ipynb`       | Hybrid retriever: BM25 + FAISS |
| `2-reranking.ipynb`         | LLM and FlashRank reranking    |
| `3-mmr.ipynb`               | Maximal Marginal Relevance     |
| `langchain_sample.txt`      | Sample data for reranking      |
| `langchain_rag_dataset.txt` | Sample data for MMR            |

---

> ğŸ’¡ **Golden Rule:** Start simple â†’ Add hybrid if missing keywords â†’ Add MMR if redundant â†’ Add reranking if need precision!
